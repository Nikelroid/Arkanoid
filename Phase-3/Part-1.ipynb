{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nikelroid/Arkanoid/blob/main/Phase-3/Part-1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee4wEwBwy9qE"
      },
      "source": [
        "# Loading data onto the disk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyaJ4h0Oy9qH",
        "outputId": "6786d6bc-5695-42fc-a47c-bc54841f135d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iZpV3lGy9qI",
        "outputId": "a145f2ec-e445-4c62-cedb-706de8f8fcd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive\n"
          ]
        }
      ],
      "source": [
        "%cd drive/My Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rc8F8EGxy9qJ"
      },
      "outputs": [],
      "source": [
        "# !ls drive/My\\ Drive/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJexWXK2y9qJ"
      },
      "outputs": [],
      "source": [
        "# !cp train_ende.zip .\n",
        "# !cp test.zip ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrY8wnUOy9qK"
      },
      "outputs": [],
      "source": [
        "#!git clone https://github.com/XL2248/MSCTD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pE0KdySQy9qK"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "!cp MSCTD/MSCTD_data/ende/english_*.txt .\n",
        "!cp MSCTD/MSCTD_data/ende/image_index_*.txt .\n",
        "!cp MSCTD/MSCTD_data/ende/sentiment_*.txt .\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "for file in os.listdir('MSCTD/MSCTD_data/ende'):\n",
        "    if file.startswith('english_'):\n",
        "        shutil.copy('MSCTD/MSCTD_data/ende/' + file, file)\n",
        "    if file.startswith('image_index_'):\n",
        "        shutil.copy('MSCTD/MSCTD_data/ende/' + file, file)\n",
        "    if file.startswith('sentiment_'):\n",
        "        shutil.copy('MSCTD/MSCTD_data/ende/' + file, file)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pL84Z09dy9qL"
      },
      "outputs": [],
      "source": [
        "#!pip install --upgrade --no-cache-dir gdown\n",
        "#!gdown --id 1GAZgPpTUBSfhne-Tp0GDkvSHuq6EMMbj\n",
        "#!gdown --id 1B9ZFmSTqfTMaqJ15nQDrRNLqBvo-B39W"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive/dataset/train"
      ],
      "metadata": {
        "id": "X36bL6FdZZ-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "qB-yhXBGLx47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifqLr4vyy9qL"
      },
      "outputs": [],
      "source": [
        "!unzip train_ende.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOjTBAoJZkM2"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "%%bash\n",
        "for x in *.zip\n",
        "do\n",
        "  unzip -qq $x\n",
        "done\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0p4n5fty9qM"
      },
      "outputs": [],
      "source": [
        "# !mkdir dataset\n",
        "# !cd dataset; mkdir train test dev\n",
        "'''\n",
        "os.makedirs('dataset', exist_ok=True)\n",
        "os.makedirs('dataset/train', exist_ok=True)\n",
        "os.makedirs('dataset/test', exist_ok=True)\n",
        "os.makedirs('dataset/dev', exist_ok=True)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XqUi8SGy9qM"
      },
      "outputs": [],
      "source": [
        "# !mv *train* dataset/train\n",
        "# !mv *test* dataset/test\n",
        "# !mv *dev* dataset/dev\n",
        "'''\n",
        "for file in os.listdir():\n",
        "    if 'train' in file:\n",
        "        shutil.move(file, 'dataset/train')\n",
        "    if 'test' in file:\n",
        "        shutil.move(file, 'dataset/test')\n",
        "    if 'dev' in file:\n",
        "        shutil.move(file, 'dataset/dev')\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnrp5n8jy9qM"
      },
      "source": [
        "# Dataset and Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kPg0W8My9qN"
      },
      "outputs": [],
      "source": [
        "!pip install mtcnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dCUd5o_Xy9qN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms as T\n",
        "from torchvision.io import read_image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "from mtcnn.mtcnn import MTCNN\n",
        "import linecache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XoQMEB-0y9qN"
      },
      "outputs": [],
      "source": [
        "class MSCTD_Dataset (Dataset):\n",
        "  def __init__(self, dataset_dir, images_dir, conversation_dir, texts, sentiments,\n",
        "                transform=None, preprocess_func=None, pad_idx=None, max_len=None):\n",
        "    self.dataset_path = Path(dataset_dir)\n",
        "    self.images_path = self.dataset_path / images_dir\n",
        "    self.sentiment_path = self.dataset_path / sentiments\n",
        "    self.text_path = self.dataset_path / texts\n",
        "    self.conversations_path = self.dataset_path / conversation_dir\n",
        "\n",
        "    self.transform = transform\n",
        "\n",
        "    self.preprocess_func = preprocess_func\n",
        "    self.pad_idx = pad_idx\n",
        "    self.max_len = max_len\n",
        "\n",
        "    with open(self.text_path, 'r') as f:\n",
        "        self.texts = f.read().splitlines()\n",
        "\n",
        "    with open(self.sentiment_path, 'r') as f:\n",
        "        self.sentiments = np.array(f.read().splitlines()).astype(\"int32\")\n",
        "\n",
        "    with open(self.conversations_path, 'r') as f:\n",
        "        self.conversations = np.array(f.read().splitlines())\n",
        "    \n",
        "  def __len__(self):\n",
        "        return len(self.sentiments)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "        img_path = self.images_path / f'{idx}.jpg'\n",
        "        image = Image.open(img_path)\n",
        "        # image = read_image(str(img_path))\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "       \n",
        "        text = self.texts[idx].strip()\n",
        "\n",
        "        if self.preprocess_func is not None:\n",
        "            text = self.preprocess_func(text)\n",
        "            if self.max_len is not None:\n",
        "                text = text[:self.max_len]\n",
        "            if self.pad_idx is not None:\n",
        "                text = F.pad(torch.tensor(text), (0, self.max_len - len(text)), 'constant', self.pad_idx)\n",
        "        \n",
        "        sentiment = self.sentiments[idx]\n",
        "\n",
        "        data_dict = {\"text\":text,\n",
        "                     \"image\":image,\n",
        "                     \"sentiment\":sentiment}\n",
        "        return data_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rPs6IzWG1ERE"
      },
      "outputs": [],
      "source": [
        "class Final_Dataset (MSCTD_Dataset):\n",
        "  def __init__(self, dataset_dir, images_dir, conversation_dir, texts, sentiments,\n",
        "               preprocess_func=None, pad_idx=None, max_len=None, transform=None):\n",
        "\n",
        "    super().__init__(dataset_dir, images_dir, conversation_dir, texts, sentiments, transform)\n",
        "    self.dataset_path = Path(dataset_dir)\n",
        "    self.images_path = self.dataset_path / images_dir\n",
        "    self.sentiment_path = self.dataset_path / sentiments\n",
        "    self.text_path = self.dataset_path / texts\n",
        "    self.conversations_path = self.dataset_path / conversation_dir\n",
        "    self.transform = transform\n",
        "    self.preprocess_func = preprocess_func\n",
        "    self.pad_idx = pad_idx\n",
        "    self.max_len = max_len\n",
        "\n",
        "    with open(self.sentiment_path, 'r') as f:\n",
        "      self.length = len(f.readlines())\n",
        "\n",
        "    with open(self.text_path, 'r') as f:\n",
        "        self.texts = f.read().splitlines()\n",
        "\n",
        "    with open(self.sentiment_path, 'r') as f:\n",
        "        self.sentiments = np.array(f.read().splitlines()).astype(\"int32\")\n",
        "    \n",
        "    with open(self.conversations_path, 'r') as f:\n",
        "        self.conversations = np.array(f.read().splitlines())\n",
        "    \n",
        "  def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        if self.preprocess_func is not None:\n",
        "            text = self.preprocess_func(text)\n",
        "            if self.max_len is not None:\n",
        "                text = text[:self.max_len]\n",
        "            if self.pad_idx is not None:\n",
        "                text = F.pad(torch.tensor(text), (0, self.max_len - len(text)), 'constant', self.pad_idx)\n",
        "        labels = self.sentiments[idx]\n",
        "        img_path = self.images_path / f'{idx}.jpg'\n",
        "        image = Image.open(img_path)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        sentiment = self.sentiments[idx]\n",
        "        return text,image,sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaGpy76hEVKb"
      },
      "outputs": [],
      "source": [
        "!pip install pyenchant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "E_CTK1b2y9qO"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "import enchant\n",
        "english_dict = enchant.Dict(\"en_US\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkzIr7yhEi5J"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "a0AAW7pHy9qO"
      },
      "outputs": [],
      "source": [
        "NUM = '<NUM>'\n",
        "UNK = '<UNK>'\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "def sent_preprocess(sent, lower=True, remove_punct=True, remove_stopwords=True,\n",
        "                    lemmatize=True, handle_nums=True, handle_unknowns=True):\n",
        "    if lower:\n",
        "        sent = sent.lower()\n",
        "    \n",
        "    if remove_punct:\n",
        "        sent = sent.translate(str.maketrans('', '', string.punctuation))\n",
        "    \n",
        "    word_tokens = word_tokenize(sent)\n",
        "\n",
        "    if remove_stopwords:\n",
        "        word_tokens = [w for w in word_tokens if not w in stop_words]\n",
        "\n",
        "    if lemmatize:\n",
        "        word_tokens = [lemmatizer.lemmatize(w) for w in word_tokens]\n",
        "\n",
        "    if handle_nums:\n",
        "        \n",
        "        def is_number(s):\n",
        "            if s.isdigit():\n",
        "                return True\n",
        "            if s[:-2].isdigit():\n",
        "                if s[-2:] == 'th' or s[-2:] == 'st' or s[-2:] == 'nd' or s[-2:] == 'rd':\n",
        "                    return True\n",
        "            return False\n",
        "\n",
        "        word_tokens = [NUM if is_number(w) else w for w in word_tokens]\n",
        "\n",
        "    if handle_unknowns:\n",
        "        word_tokens = [w if english_dict.check(w) else UNK for w in word_tokens]\n",
        "\n",
        "    return word_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d33pQ5Yiy9qP"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwNin1RRy9qP"
      },
      "source": [
        "### Bert Congfiguration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgl3YjRdy9qQ"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "N4Ucqi-_y9qQ"
      },
      "outputs": [],
      "source": [
        "from transformers import BertConfig, BertTokenizer\n",
        "from transformers import BertModel, AutoModel, BertForSequenceClassification\n",
        "from transformers import AdamW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "O2Bh8S8cy9qQ"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "0qcfGt4Iy9qR"
      },
      "outputs": [],
      "source": [
        "# general config\n",
        "MAX_LEN = 30\n",
        "\n",
        "TRAIN_BATCH_SIZE = 512\n",
        "VALID_BATCH_SIZE = 512\n",
        "TEST_BATCH_SIZE = 512\n",
        "\n",
        "EPOCHS = 4\n",
        "LEARNING_RATE = 5e-5\n",
        "\n",
        "MODEL_NAME = 'bert-base-uncased'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWZolUFby9qR"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
        "config = BertConfig.from_pretrained(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "D-5euGEWy9qR"
      },
      "outputs": [],
      "source": [
        "def bert_preprocess(text):\n",
        "    return tokenizer.encode_plus(\n",
        "        text,\n",
        "        max_length=MAX_LEN,\n",
        "        truncation=True,\n",
        "        add_special_tokens=True,\n",
        "        return_token_type_ids=True,\n",
        "        return_attention_mask=True,\n",
        "        padding='max_length',\n",
        "        return_tensors='pt',\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "RZ0HjhmEy9qR"
      },
      "outputs": [],
      "source": [
        "# change this if needed\n",
        "transform = T.Compose([T.ToTensor()])\n",
        "\n",
        "trainset = MSCTD_Dataset('dataset/train', 'train_ende', 'image_index_train.txt', 'english_train.txt', 'sentiment_train.txt', preprocess_func=bert_preprocess, transform=transform)\n",
        "devset = MSCTD_Dataset('dataset/dev', 'dev', 'image_index_dev.txt', 'english_dev.txt', 'sentiment_dev.txt', preprocess_func=bert_preprocess, transform=transform)\n",
        "testset = MSCTD_Dataset('dataset/test', 'test', 'image_index_test.txt', 'english_test.txt', 'sentiment_test.txt', preprocess_func=bert_preprocess, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "7CCPafb8y9qR"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(trainset, batch_size=32, shuffle=False)\n",
        "dev_loader = DataLoader(devset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(testset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daeEHjMa-eVH"
      },
      "source": [
        "# Get and save embedding of models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "pxtujlOnLJ4E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7184785-916a-408e-d799-c7ae108c6cac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "zdTEdyP-LJ36"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([transforms.ToTensor()\n",
        "                                ,transforms.Resize((288,288),transforms.InterpolationMode(\"bicubic\"))\n",
        "                                ,transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])])     \n",
        "def bert_preprocess(text):\n",
        "    return tokenizer.encode_plus(\n",
        "        text,\n",
        "        max_length= MAX_LEN,\n",
        "        truncation=True,\n",
        "        add_special_tokens=True,\n",
        "        return_token_type_ids=True,\n",
        "        return_attention_mask=True,\n",
        "        padding='max_length',\n",
        "        return_tensors='pt',\n",
        "    )\n",
        "train_dataset = Final_Dataset('dataset/train', 'train_ende', 'image_index_train.txt', 'english_train.txt', 'sentiment_train.txt',preprocess_func=bert_preprocess,transform=transform)\n",
        "dev_dataset = Final_Dataset('dataset/dev', 'dev', 'image_index_dev.txt', 'english_dev.txt', 'sentiment_dev.txt',preprocess_func=bert_preprocess,transform=transform)\n",
        "test_dataset = Final_Dataset('dataset/test', 'test', 'image_index_test.txt', 'english_test.txt', 'sentiment_test.txt',preprocess_func=bert_preprocess,transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "NvGKsWqoLJ4F"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
        "dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=VALID_BATCH_SIZE, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0WcDbRlStCn"
      },
      "outputs": [],
      "source": [
        "#!pip install --upgrade --no-cache-dir gdown\n",
        "#!gdown 11caq-CNLP6_V3106zj0zAkFyYPAjyjyw\n",
        "\n",
        "!pip install transformers\n",
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "def load_pretrained_bert(name='models/bert_model.pt'):\n",
        "    model = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "        num_labels = 3,\n",
        "        output_attentions = False,\n",
        "        output_hidden_states = False,\n",
        "    ).to(device)\n",
        "    model.load_state_dict(torch.load(name))\n",
        "    model.classifier = nn.Sequential()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install --upgrade --no-cache-dir gdown\n",
        "#!gdown 1EfyDFNxAHGjvnLPRbP0SfnkV9g33-CFJ\n",
        "\n",
        "from torchvision.models import efficientnet_b2, EfficientNet_B2_Weights\n",
        "\n",
        "def load_pretrained_image(name = 'models/scene_modal_en.pth'):\n",
        "    class lastLayer(nn.Module):\n",
        "      def __init__(self, pretrained):\n",
        "          super(lastLayer, self).__init__()\n",
        "          self.pretrained = pretrained\n",
        "          self.last = nn.Sequential(\n",
        "              nn.Dropout(p = 0.2,inplace=True),\n",
        "              nn.Linear(1408, 90),\n",
        "              nn.Dropout(p = 0.3,inplace=True),\n",
        "              nn.Linear(90, 30),\n",
        "              nn.Dropout(p = 0.1,inplace=True),\n",
        "              nn.Linear(30, 3),\n",
        "              )\n",
        "      \n",
        "      def forward(self, x):\n",
        "          x = self.pretrained(x)\n",
        "          x = self.last(x)\n",
        "          return x\n",
        "\n",
        "    image_model = efficientnet_b2(weights=EfficientNet_B2_Weights.IMAGENET1K_V1)\n",
        "    image_model.classifier = nn.Sequential()\n",
        "    image_model = lastLayer(image_model).to(device)\n",
        "    image_model.load_state_dict(torch.load(\"models/scene_modal_en.pth\"))\n",
        "    image_model.last = nn.Sequential()\n",
        "    return image_model"
      ],
      "metadata": {
        "id": "PPgwDx9TS7od"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_model = load_pretrained_image()\n",
        "text_model = load_pretrained_bert()\n"
      ],
      "metadata": {
        "id": "Ny_eeNyAWYCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!mkdir dataset/train/cated_data"
      ],
      "metadata": {
        "id": "0n-5ynunCjSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4_eC_ltRDOM"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "import tqdm\n",
        "import pickle\n",
        "\n",
        "vectors = []\n",
        "labels = []\n",
        "with tqdm.tqdm(enumerate(train_loader), total=len(train_loader)) as pbar:\n",
        "      with torch.no_grad():  \n",
        "        for i, m in pbar:\n",
        "            data_i,image_i, y = m\n",
        "            (input_ids, attention_mask, token_type_ids) = data_i.values()\n",
        "            input_ids, attention_mask, token_type_ids = input_ids.to(device), attention_mask.to(device), token_type_ids.to(device)\n",
        "            y = y.to(device)\n",
        "            input_ids = input_ids.squeeze(1)\n",
        "            attention_mask = attention_mask.squeeze(1)\n",
        "            token_type_ids = token_type_ids.squeeze(1)\n",
        "            output = text_model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "            rep_text = output.logits\n",
        "            y = y.to(device)\n",
        "            image_i = image_i.to(device)\n",
        "            rep_image = image_model(image_i)\n",
        "            rep = torch.cat((rep_text,rep_image),dim=1)\n",
        "            print(rep.size())\n",
        "            print(y.size())\n",
        "            torch.save(y, \"dataset/train/cated_data/labels_\"+str(i)+\".pt\") \n",
        "            torch.save(rep, \"dataset/train/cated_data/vectors_\"+str(i)+\".pt\") \n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!mkdir dataset/test/cated_data"
      ],
      "metadata": {
        "id": "eG34FLRtJUSx"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import tqdm\n",
        "import pickle\n",
        "\n",
        "vectors = []\n",
        "labels = []\n",
        "with tqdm.tqdm(enumerate(test_loader), total=len(test_loader)) as pbar:\n",
        "      with torch.no_grad():  \n",
        "        for i, m in pbar:\n",
        "            data_i,image_i, y = m\n",
        "            (input_ids, attention_mask, token_type_ids) = data_i.values()\n",
        "            input_ids, attention_mask, token_type_ids = input_ids.to(device), attention_mask.to(device), token_type_ids.to(device)\n",
        "            y = y.to(device)\n",
        "            input_ids = input_ids.squeeze(1)\n",
        "            attention_mask = attention_mask.squeeze(1)\n",
        "            token_type_ids = token_type_ids.squeeze(1)\n",
        "            output = text_model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "            rep_text = output.logits\n",
        "            y = y.to(device)\n",
        "            image_i = image_i.to(device)\n",
        "            rep_image = image_model(image_i)\n",
        "            rep = torch.cat((rep_text,rep_image),dim=1)\n",
        "            print(rep.size())\n",
        "            print(y.size())\n",
        "            torch.save(y, \"dataset/test/cated_data/labels_\"+str(i)+\".pt\") \n",
        "            torch.save(rep, \"dataset/test/cated_data/vectors_\"+str(i)+\".pt\") \n",
        "'''"
      ],
      "metadata": {
        "id": "2jA7MPKyTCSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_lenght = int(len([name for name in os.listdir(\"dataset/train/cated_data\") if os.path.isfile(os.path.join(\"dataset/train/cated_data\", name))])/2)\n",
        "data_train = torch.load(\"dataset/train/cated_data/vectors_\"+str(0)+\".pt\")\n",
        "labels_train = torch.load(\"dataset/train/cated_data/labels_\"+str(0)+\".pt\")\n",
        "for i in range(1,image_lenght):\n",
        "  t = torch.load(\"dataset/train/cated_data/vectors_\"+str(i)+\".pt\")\n",
        "  lb = torch.load(\"dataset/train/cated_data/labels_\"+str(i)+\".pt\")\n",
        "  data_train = torch.cat((data_train,t),dim=0)\n",
        "  labels_train = torch.cat((labels_train,lb),dim=0)"
      ],
      "metadata": {
        "id": "2XDFwaRu-KPr"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train.size()"
      ],
      "metadata": {
        "id": "b1XxsJ59CWRf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14443b61-b8eb-4b04-dac2-5c98d1be2dcb"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([20240, 2176])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels_train.size()"
      ],
      "metadata": {
        "id": "YMIyuKUYDLMA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "307d40ca-4657-4668-84ba-341229127756"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([20240])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_lenght = int(len([name for name in os.listdir(\"dataset/test/cated_data\") if os.path.isfile(os.path.join(\"dataset/test/cated_data\", name))])/2)\n",
        "data_test = torch.load(\"dataset/test/cated_data/vectors_\"+str(0)+\".pt\")\n",
        "labels_test = torch.load(\"dataset/test/cated_data/labels_\"+str(0)+\".pt\")\n",
        "for i in range(1,image_lenght):\n",
        "  t = torch.load(\"dataset/test/cated_data/vectors_\"+str(i)+\".pt\")\n",
        "  lb = torch.load(\"dataset/test/cated_data/labels_\"+str(i)+\".pt\")\n",
        "  data_test = torch.cat((data_test,t),dim=0)\n",
        "  labels_test = torch.cat((labels_test,lb),dim=0)"
      ],
      "metadata": {
        "id": "oBEdrV---KL-"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test.size()"
      ],
      "metadata": {
        "id": "fWkfwEVeESOq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bc5f35e-d2cd-4fea-e301-9eadc23be522"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5067, 2176])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels_test"
      ],
      "metadata": {
        "id": "BuX36_TiESOr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "373b94d2-7aa1-4f4b-8439-d02e5ad43100"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 1, 2,  ..., 2, 1, 2], device='cuda:0', dtype=torch.int32)"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9DBYSgfy9qS"
      },
      "source": [
        "### Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "id": "Syjw9cQCy9qS"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "class MultiModalModel(nn.Module):\n",
        "        def __init__(self,num_input,num_classes=3):\n",
        "          super(MultiModalModel,self).__init__()\n",
        "          hidden_1 = num_input//2\n",
        "          self.fc1 = nn.Linear(num_input, hidden_1)\n",
        "          self.fc2 = nn.Linear(hidden_1, num_classes)\n",
        "          self.droput = nn.Dropout(0.1)\n",
        "\n",
        "        def forward(self,x):\n",
        "          x = F.relu(self.fc1(x))\n",
        "          x = self.droput(x)\n",
        "          x = self.fc2(x)\n",
        "          return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modalmodel = MultiModalModel(data_train.size()[1]).to(device)"
      ],
      "metadata": {
        "id": "u6-Iw2n7-ByU"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmEPjcvIy9qS"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "OYgIeazay9qS"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "pAn3Vcziy9qS"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "\n",
        "def one_epoch(model, vector,truth, criterion, optimizer=None, train=True,batch_size = 64):\n",
        "    total_loss = 0\n",
        "    N = len(truth)\n",
        "    Y = []\n",
        "    Y_pred = []\n",
        "    if train:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "    for i in (range(N//batch_size + 1)):\n",
        "        vec = vector[i*batch_size:min((i+1)*batch_size,N)]\n",
        "        y = truth[i*batch_size:min((i+1)*batch_size,N)]\n",
        "        \n",
        "        if train:\n",
        "            optimizer.zero_grad()\n",
        "        vec = vec.to(device)\n",
        "        y = y.long().to(device)\n",
        "        p = model(vec)\n",
        "        loss = criterion(p, y.long())\n",
        "        total_loss += loss.item() * len(y)\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        y_pred = p.argmax(dim=-1)\n",
        "        Y.append(y.cpu().numpy())\n",
        "        Y_pred.append(y_pred.cpu().numpy())\n",
        "\n",
        "    total_loss /= N\n",
        "\n",
        "    Y = np.concatenate(Y)\n",
        "    Y_pred = np.concatenate(Y_pred)\n",
        "    acc = accuracy_score(Y_pred, Y)\n",
        "    result = {'loss': total_loss, 'accuracy': acc}\n",
        "    if metrics is not None:\n",
        "        result.update({metric: metric_func(Y, Y_pred) for metric, metric_func in metrics.items()})\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "id": "ngHxX_Y7y9qT"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train,test, num_epochs, criterion, optimizer, model_name='pytroch-model', scheduler=None,batch_size=64):\n",
        "    train_loader, train_labels = train\n",
        "    test_loader, test_labels = test\n",
        "    min_val_acc = 0\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accuracies = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        result = one_epoch(model, train_loader,train_labels, criterion, optimizer, train=True,batch_size=batch_size)\n",
        "        train_loss = result['loss']\n",
        "        train_acc = result['accuracy']\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "        val_result = one_epoch(model, test_loader,test_labels, criterion, train=False,batch_size=batch_size)\n",
        "        val_loss = val_result['loss']\n",
        "        val_acc = val_result['accuracy']\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_acc)\n",
        "        \n",
        "        print()\n",
        "        print(f'Train Accuracy : {train_acc*100.:2f}% , Train Loss : {train_loss:4f}')\n",
        "        print(f'Valid Accuracy : {val_acc*100.:2f}% , Valid Loss : {val_loss:4f}')\n",
        "\n",
        "        if val_acc > min_val_acc:\n",
        "            min_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), f'{model_name}.pt')\n",
        "\n",
        "        if scheduler:\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "    plt.plot(train_losses, label='train')\n",
        "    plt.plot(val_losses, label='val')\n",
        "    plt.title('Loss history of training and Val sets')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(train_accuracies, label='train')\n",
        "    plt.plot(val_accuracies, label='val')\n",
        "    plt.title('Accuracy history of training and Val sets')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    model.load_state_dict(torch.load(f'{model_name}.pt'))\n",
        "    return model, min_val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "p3hzyeR0y9qT"
      },
      "outputs": [],
      "source": [
        "average_policy = 'macro'\n",
        "metrics = {'accuracy': accuracy_score, 'precision': lambda y1, y2: precision_score(y1, y2, average=average_policy),\n",
        "           'recall': lambda y1, y2: recall_score(y1, y2, average=average_policy),\n",
        "           'f1': lambda y1, y2: f1_score(y1, y2, average=average_policy),\n",
        "           'confusion_matrix': confusion_matrix}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-mbbeDiy9qT"
      },
      "outputs": [],
      "source": [
        "# Training Configuration\n",
        "LEARNING_RATE = 1e-5\n",
        "EPOCH = 30\n",
        "BATCH_SIZE = 64\n",
        "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, modalmodel.parameters()), lr=LEARNING_RATE)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2, verbose=True, factor=0.5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "modalmodel, min_val_loss = train_model(modalmodel, (data_train,labels_train),(data_test,labels_test),EPOCH, criterion, optimizer, model_name='multi_modal', scheduler=scheduler,batch_size =BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-1BiHYgy9qT"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "Xzj1ybzLy9qU"
      },
      "outputs": [],
      "source": [
        "def eval_model(model, loader, metrics=metrics, set_name='Test', plot_confusion_matrix=True):\n",
        "    test_loader,test_labels = loader\n",
        "    results = one_epoch(modalmodel, test_loader,test_labels, criterion, train=False,batch_size=64)\n",
        "    disp = ConfusionMatrixDisplay(results.pop('confusion_matrix'))\n",
        "    if plot_confusion_matrix:\n",
        "        disp.plot()\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "id": "cEzw44yBy9qU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "outputId": "5bc6a1fa-9043-4c8b-f28e-6843ffed704e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss': 1.0308159757512125,\n",
              " 'accuracy': 0.5109532267613973,\n",
              " 'precision': 0.49459412570910005,\n",
              " 'recall': 0.49214243052451795,\n",
              " 'f1': 0.4865680397937809}"
            ]
          },
          "metadata": {},
          "execution_count": 192
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "eval_model(modalmodel, (data_test,labels_test))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "67bfac4f4aefe1c16f1836a62d55b6e6baa7aba1ac5ce70e93ee8e90eb4f073a"
      }
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}